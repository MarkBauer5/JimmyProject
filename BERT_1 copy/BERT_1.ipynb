{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article: [Predicting TCR-Epitope Binding Specificity Using Deep Metric Learning and Multimodal Learning](https://www.mdpi.com/2073-4425/12/4/572)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objects**\n",
    "\n",
    "*1. Develop a Computational Model: The paper aims to create a convolutional neural network model that utilizes deep metric learning and multimodal learning techniques to predict interactions between T cell receptors (TCRs) and Major Histocompatibility Complex class I-peptide complexes (pMHC).*\n",
    "\n",
    "*2. Simultaneous TCR-Epitope Binding Prediction: The paper seeks to perform two critical tasks in TCR-epitope binding prediction: identifying the TCRs that bind a given epitope from a TCR repertoire and identifying the binding epitope of a given TCR from a list of candidate epitopes. The goal is to achieve accurate predictions for both tasks simultaneously.*\n",
    "\n",
    "*3. Gain Insights into Binding Specificity: The paper aims to provide insights into the factors that determine TCR-epitope binding specificity, including the identification of key amino acid sequence patterns and positions within the TCR that are important for binding specificity. Additionally, the paper challenges the assumption that physical proximity to epitopes is the sole determinant of TCR-epitope specificity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 17:40:31.220459: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 17:40:31.411610: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-30 17:40:31.411629: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-30 17:40:31.448037: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-30 17:40:32.256073: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 17:40:32.256151: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 17:40:32.256159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot enocoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 17:40:36.056021: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-30 17:40:36.056071: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-30 17:40:36.056096: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (matts-computer): /proc/driver/nvidia/version does not exist\n",
      "2023-10-30 17:40:36.056537: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "negatives = pd.read_csv(\"./negative.csv\")\n",
    "\n",
    "amino_acids = []\n",
    "\n",
    "def add_to_amino_acids(a_sequence: str):\n",
    "    for acid in a_sequence:\n",
    "        if acid not in amino_acids:\n",
    "            amino_acids.append(acid)\n",
    "\n",
    "positives.stack().reset_index(drop=True).apply(add_to_amino_acids)\n",
    "\n",
    "amino_acids.sort()\n",
    "\n",
    "amino_acid_label_encoder = LabelEncoder()\n",
    "amino_acid_label_encoder.fit(amino_acids)\n",
    "\n",
    "all_amino_acids = amino_acid_label_encoder.transform(amino_acids)\n",
    "\n",
    "def feature_map(p_sequence):\n",
    "    return [tf.one_hot(amino_acid_label_encoder.transform(list(x)), len(all_amino_acids)) for x in p_sequence]\n",
    "\n",
    "data_cd3r = feature_map(positives[\"cdr3\"])\n",
    "data_epitope = feature_map(positives[\"antigen.epitope\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"gene.jpg\" alt=\"Figure 1\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CDR3B and Epitope Sequence Representation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Representation Goals:**\n",
    "\n",
    "  1. Convert amino acid sequences from string format to a numeric representation.\n",
    "  2. Develop a numerical procedure utilizing Atchley representation to capture physical and biochemical properties.\n",
    "  3. Create matrices with specified dimensions through padding to accommodate varying sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Construction with the Atchley Representation in both CDR3B and the Epitope\n",
    "Fixed: Instead of constructing the sentences manually, I constructed the sentences utilizing the BertTokenizer in relateion to the amino_aciv_vocab.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]]])\n",
      "tensor([[[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "def convert_to_space_separated_string(series):\n",
    "    return ' '.join(series)\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"./amino_acid_vocab.txt\")\n",
    "\n",
    "def construct_sentences(dataframe):\n",
    "    cdr3_sentences = dataframe[\"cdr3\"]\n",
    "    epitope_sentences = dataframe[\"antigen.epitope\"]\n",
    "    return cdr3_sentences, epitope_sentences\n",
    "\n",
    "def pad_sentences(sentences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks)\n",
    "\n",
    "max_length = 32\n",
    "\n",
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "\n",
    "for column in positives.columns:\n",
    "    positives[column] = positives[column].apply(convert_to_space_separated_string)\n",
    "\n",
    "cdr3_sentences, epitope_sentences = construct_sentences(positives)\n",
    "\n",
    "cdr3_input_ids, cdr3_attention_masks = pad_sentences(cdr3_sentences, max_length)\n",
    "epitope_input_ids, epitope_attention_masks = pad_sentences(epitope_sentences, max_length)\n",
    "\n",
    "cdr3_combined = torch.cat((cdr3_input_ids, cdr3_attention_masks), dim=1)\n",
    "epitope_combined = torch.cat((epitope_input_ids, epitope_attention_masks), dim=1)\n",
    "\n",
    "cdr3_train_data, cdr3_test_data = train_test_split(cdr3_combined, test_size=0.2, random_state=42)\n",
    "epitope_train_data, epitope_test_data = train_test_split(epitope_combined, test_size=0.2, random_state=42)\n",
    "print(cdr3_input_ids)\n",
    "print(epitope_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure encoding CDR3B and Eptiope Amino Acid Sequences as Numerical Matrices\n",
    "#### Taken from Matt's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(25, 32, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 32)\n",
       "      (token_type_embeddings): Embedding(2, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=32, out_features=25, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "config = BertConfig.from_json_file(\"./bert_config.json\")\n",
    "model = BertForMaskedLM(config=config)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cdr3_train_data = cdr3_train_data.to(device)\n",
    "cdr3_test_data = cdr3_test_data.to(device)\n",
    "epitope_train_data = epitope_train_data.to(device)\n",
    "epitope_test_data = epitope_test_data.to(device)\n",
    "\n",
    "model.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
