{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article: [Predicting TCR-Epitope Binding Specificity Using Deep Metric Learning and Multimodal Learning](https://www.mdpi.com/2073-4425/12/4/572)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objects**\n",
    "\n",
    "#### *1. Develop a Computational Model: The paper aims to create a convolutional neural network model that utilizes deep metric learning and multimodal learning techniques to predict interactions between T cell receptors (TCRs) and Major Histocompatibility Complex class I-peptide complexes (pMHC).*\n",
    "\n",
    "#### *2. Simultaneous TCR-Epitope Binding Prediction: The paper seeks to perform two critical tasks in TCR-epitope binding prediction: identifying the TCRs that bind a given epitope from a TCR repertoire and identifying the binding epitope of a given TCR from a list of candidate epitopes. The goal is to achieve accurate predictions for both tasks simultaneously.*\n",
    "\n",
    "##### *3. Gain Insights into Binding Specificity: The paper aims to provide insights into the factors that determine TCR-epitope binding specificity, including the identification of key amino acid sequence patterns and positions within the TCR that are important for binding specificity. Additionally, the paper challenges the assumption that physical proximity to epitopes is the sole determinant of TCR-epitope specificity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot enocoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "negatives = pd.read_csv(\"./negative.csv\")\n",
    "\n",
    "amino_acids = []\n",
    "\n",
    "def add_to_amino_acids(a_sequence: str):\n",
    "    for acid in a_sequence:\n",
    "        if acid not in amino_acids:\n",
    "            amino_acids.append(acid)\n",
    "\n",
    "positives.stack().reset_index(drop=True).apply(add_to_amino_acids)\n",
    "\n",
    "amino_acids.sort()\n",
    "\n",
    "amino_acid_label_encoder = LabelEncoder()\n",
    "amino_acid_label_encoder.fit(amino_acids)\n",
    "\n",
    "all_amino_acids = amino_acid_label_encoder.transform(amino_acids)\n",
    "\n",
    "def feature_map(p_sequence):\n",
    "    return [tf.one_hot(amino_acid_label_encoder.transform(list(x)), len(all_amino_acids)) for x in p_sequence]\n",
    "\n",
    "data_cd3r = feature_map(positives[\"cdr3\"])\n",
    "data_epitope = feature_map(positives[\"antigen.epitope\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"gene.jpg\" alt=\"Figure 1\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CDR3B and Epitope Sequence Representation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Representation Goals:**\n",
    "\n",
    "  1. Convert amino acid sequences from string format to a numeric representation.\n",
    "  2. Develop a numerical procedure utilizing Atchley representation to capture physical and biochemical properties.\n",
    "  3. Create matrices with specified dimensions through padding to accommodate varying sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Construction with the Atchley Representation in both CDR3B and the Epitope\n",
    "Fixed: Instead of constructing the sentences manually, I constructed the sentences utilizing the BertTokenizer in relateion to the amino_aciv_vocab.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure encoding CDR3B and Eptiope Amino Acid Sequences as Numerical Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]]])\n",
      "tensor([[[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "def convert_to_space_separated_string(series):\n",
    "    return ' '.join(series)\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"./amino_acid_vocab.txt\")\n",
    "\n",
    "def construct_sentences(dataframe):\n",
    "    cdr3_sentences = dataframe[\"cdr3\"]\n",
    "    epitope_sentences = dataframe[\"antigen.epitope\"]\n",
    "    return cdr3_sentences, epitope_sentences\n",
    "\n",
    "def pad_sentences(sentences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks)\n",
    "\n",
    "max_length = 32\n",
    "\n",
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "\n",
    "for column in positives.columns:\n",
    "    positives[column] = positives[column].apply(convert_to_space_separated_string)\n",
    "\n",
    "cdr3_sentences, epitope_sentences = construct_sentences(positives)\n",
    "\n",
    "cdr3_input_ids, cdr3_attention_masks = pad_sentences(cdr3_sentences, max_length)\n",
    "epitope_input_ids, epitope_attention_masks = pad_sentences(epitope_sentences, max_length)\n",
    "\n",
    "cdr3_combined = torch.cat((cdr3_input_ids, cdr3_attention_masks), dim=1)\n",
    "epitope_combined = torch.cat((epitope_input_ids, epitope_attention_masks), dim=1)\n",
    "\n",
    "cdr3_train_data, cdr3_test_data = train_test_split(cdr3_combined, test_size=0.2, random_state=42)\n",
    "epitope_train_data, epitope_test_data = train_test_split(epitope_combined, test_size=0.2, random_state=42)\n",
    "print(cdr3_input_ids)\n",
    "print(epitope_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(25, 32, padding_idx=0)\n",
       "      (position_embeddings): Embedding(128, 32)\n",
       "      (token_type_embeddings): Embedding(2, 32)\n",
       "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "              (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=32, out_features=25, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "config = BertConfig.from_json_file(\"./bert_config.json\")\n",
    "\n",
    "model = BertForMaskedLM(config=config) # change this line and flag what we're using\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cdr3_train_data = cdr3_train_data.to(device)\n",
    "cdr3_test_data = cdr3_test_data.to(device)\n",
    "epitope_train_data = epitope_train_data.to(device)\n",
    "epitope_test_data = epitope_test_data.to(device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss Function:\n",
    "Article: [PyTorch Metric Learning](https://kevinmusgrave.github.io/pytorch-metric-learning/#:~:text=This%20customized%20triplet%20loss%20has,than%200.3%20will%20be%20discarded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.reducers import ThresholdReducer\n",
    "from pytorch_metric_learning.regularizers import LpRegularizer\n",
    "from pytorch_metric_learning import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = losses.TripletMarginLoss(distance = CosineSimilarity(), reducer = ThresholdReducer(high=0.3), embedding_regularizer = LpRegularizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the embedding between the CDR3 and the Epitope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BertForPreTraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "/Users/ceejayarana/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:119: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Creating features from dataset file at ./amino_acid_vocab.txt\n",
      "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 12736\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2\n",
      "  Number of trainable parameters = 109,514,298\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.06it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2.194, 'train_samples_per_second': 11.395, 'train_steps_per_second': 0.912, 'train_loss': 0.6165385246276855, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=0.6165385246276855, metrics={'train_runtime': 2.194, 'train_samples_per_second': 11.395, 'train_steps_per_second': 0.912, 'train_loss': 0.6165385246276855, 'epoch': 1.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForMaskedLM, LineByLineTextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Define your model configuration\n",
    "from transformers import BertConfig\n",
    "\n",
    "model_config = BertConfig(\n",
    "    vocab_size=30522,                # Common BERT vocab size\n",
    "    hidden_size=768,                # Standard BERT hidden size\n",
    "    num_hidden_layers=12,           # Standard BERT number of hidden layers\n",
    "    num_attention_heads=12,         # Standard BERT number of attention heads\n",
    "    intermediate_size=3072,         # Standard BERT intermediate size\n",
    "    max_position_embeddings=512,    # Maximum position embeddings in BERT-base\n",
    ")\n",
    "\n",
    "\n",
    "# Create a custom BERT model\n",
    "model = BertForMaskedLM(config=model_config)\n",
    "\n",
    "# Load and preprocess your text data from a CSV file\n",
    "data = pd.read_csv(\"./positive.csv\")\n",
    "text_data = data[\"antigen.epitope\"].tolist()\n",
    "\n",
    "# Join the text data into a single string, separated by newlines\n",
    "train_data = \"\\n\".join(text_data)\n",
    "\n",
    "\n",
    "# Tokenize the text data\n",
    "input_ids = tokenizer.encode(train_data, add_special_tokens=True, return_tensors=\"pt\")\n",
    "labels = input_ids.clone()\n",
    "\n",
    "# Use the \"amino_acid_vocab.txt\" file as a placeholder for your training data\n",
    "dataset = LineByLineTextDataset(tokenizer=tokenizer, file_path=\"./amino_acid_vocab.txt\", block_size=128)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-pretraining\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=10_000,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. BertForNextPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
