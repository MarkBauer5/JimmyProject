{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article: [Predicting TCR-Epitope Binding Specificity Using Deep Metric Learning and Multimodal Learning](https://www.mdpi.com/2073-4425/12/4/572)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objects**\n",
    "\n",
    "#### *1. Develop a Computational Model: The paper aims to create a convolutional neural network model that utilizes deep metric learning and multimodal learning techniques to predict interactions between T cell receptors (TCRs) and Major Histocompatibility Complex class I-peptide complexes (pMHC).*\n",
    "\n",
    "#### *2. Simultaneous TCR-Epitope Binding Prediction: The paper seeks to perform two critical tasks in TCR-epitope binding prediction: identifying the TCRs that bind a given epitope from a TCR repertoire and identifying the binding epitope of a given TCR from a list of candidate epitopes. The goal is to achieve accurate predictions for both tasks simultaneously.*\n",
    "\n",
    "##### *3. Gain Insights into Binding Specificity: The paper aims to provide insights into the factors that determine TCR-epitope binding specificity, including the identification of key amino acid sequence patterns and positions within the TCR that are important for binding specificity. Additionally, the paper challenges the assumption that physical proximity to epitopes is the sole determinant of TCR-epitope specificity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot enocoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "negatives = pd.read_csv(\"./negative.csv\")\n",
    "\n",
    "amino_acids = []\n",
    "\n",
    "def add_to_amino_acids(a_sequence: str):\n",
    "    for acid in a_sequence:\n",
    "        if acid not in amino_acids:\n",
    "            amino_acids.append(acid)\n",
    "\n",
    "positives.stack().reset_index(drop=True).apply(add_to_amino_acids)\n",
    "\n",
    "amino_acids.sort()\n",
    "\n",
    "amino_acid_label_encoder = LabelEncoder()\n",
    "amino_acid_label_encoder.fit(amino_acids)\n",
    "\n",
    "all_amino_acids = amino_acid_label_encoder.transform(amino_acids)\n",
    "\n",
    "def feature_map(p_sequence):\n",
    "    return [tf.one_hot(amino_acid_label_encoder.transform(list(x)), len(all_amino_acids)) for x in p_sequence]\n",
    "\n",
    "data_cd3r = feature_map(positives[\"cdr3\"])\n",
    "data_epitope = feature_map(positives[\"antigen.epitope\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"gene.jpg\" alt=\"Figure 1\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CDR3B and Epitope Sequence Representation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Representation Goals:**\n",
    "\n",
    "  1. Convert amino acid sequences from string format to a numeric representation.\n",
    "  2. Develop a numerical procedure utilizing Atchley representation to capture physical and biochemical properties.\n",
    "  3. Create matrices with specified dimensions through padding to accommodate varying sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Construction with the Atchley Representation in both CDR3B and the Epitope\n",
    "Fixed: Instead of constructing the sentences manually, I constructed the sentences utilizing the BertTokenizer in relateion to the amino_aciv_vocab.txt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure encoding CDR3B and Eptiope Amino Acid Sequences as Numerical Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]]])\n",
      "tensor([[[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "def convert_to_space_separated_string(series):\n",
    "    return ' '.join(series)\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"./amino_acid_vocab.txt\")\n",
    "\n",
    "def construct_sentences(dataframe):\n",
    "    cdr3_sentences = dataframe[\"cdr3\"]\n",
    "    epitope_sentences = dataframe[\"antigen.epitope\"]\n",
    "    return cdr3_sentences, epitope_sentences\n",
    "\n",
    "def pad_sentences(sentences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks)\n",
    "\n",
    "max_length = 32\n",
    "\n",
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "\n",
    "for column in positives.columns:\n",
    "    positives[column] = positives[column].apply(convert_to_space_separated_string)\n",
    "\n",
    "cdr3_sentences, epitope_sentences = construct_sentences(positives)\n",
    "\n",
    "cdr3_input_ids, cdr3_attention_masks = pad_sentences(cdr3_sentences, max_length)\n",
    "epitope_input_ids, epitope_attention_masks = pad_sentences(epitope_sentences, max_length)\n",
    "\n",
    "cdr3_combined = torch.cat((cdr3_input_ids, cdr3_attention_masks), dim=1)\n",
    "epitope_combined = torch.cat((epitope_input_ids, epitope_attention_masks), dim=1)\n",
    "\n",
    "cdr3_train_data, cdr3_test_data = train_test_split(cdr3_combined, test_size=0.2, random_state=42)\n",
    "epitope_train_data, epitope_test_data = train_test_split(epitope_combined, test_size=0.2, random_state=42)\n",
    "print(cdr3_input_ids)\n",
    "print(epitope_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ceejayarana/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/Users/ceejayarana/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "/Users/ceejayarana/anaconda3/envs/tf/lib/python3.10/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "config = BertConfig.from_json_file(\"./bert_config.json\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cdr3_train_data = cdr3_train_data.to(device)\n",
    "cdr3_test_data = cdr3_test_data.to(device)\n",
    "epitope_train_data = epitope_train_data.to(device)\n",
    "epitope_test_data = epitope_test_data.to(device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triplet Loss Function:\n",
    "Article: [PyTorch Metric Learning](https://kevinmusgrave.github.io/pytorch-metric-learning/#:~:text=This%20customized%20triplet%20loss%20has,than%200.3%20will%20be%20discarded.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "from pytorch_metric_learning.reducers import ThresholdReducer\n",
    "from pytorch_metric_learning.regularizers import LpRegularizer\n",
    "from pytorch_metric_learning import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = losses.TripletMarginLoss(distance = CosineSimilarity(), reducer = ThresholdReducer(high=0.3), embedding_regularizer = LpRegularizer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the embedding between the CDR3 and the Epitope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BertForPreTraining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. BertForNextPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
