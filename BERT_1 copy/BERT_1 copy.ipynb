{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article: [Predicting TCR-Epitope Binding Specificity Using Deep Metric Learning and Multimodal Learning](https://www.mdpi.com/2073-4425/12/4/572)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Objects**\n",
    "\n",
    "*1. Develop a Computational Model: The paper aims to create a convolutional neural network model that utilizes deep metric learning and multimodal learning techniques to predict interactions between T cell receptors (TCRs) and Major Histocompatibility Complex class I-peptide complexes (pMHC).*\n",
    "\n",
    "*2. Simultaneous TCR-Epitope Binding Prediction: The paper seeks to perform two critical tasks in TCR-epitope binding prediction: identifying the TCRs that bind a given epitope from a TCR repertoire and identifying the binding epitope of a given TCR from a list of candidate epitopes. The goal is to achieve accurate predictions for both tasks simultaneously.*\n",
    "\n",
    "*3. Gain Insights into Binding Specificity: The paper aims to provide insights into the factors that determine TCR-epitope binding specificity, including the identification of key amino acid sequence patterns and positions within the TCR that are important for binding specificity. Additionally, the paper challenges the assumption that physical proximity to epitopes is the sole determinant of TCR-epitope specificity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:17:55.384688: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 18:17:55.560325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-30 18:17:55.560346: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-30 18:17:55.597976: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-30 18:17:56.396291: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 18:17:56.396362: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 18:17:56.396368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot enocoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 18:18:00.016469: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-10-30 18:18:00.016496: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-10-30 18:18:00.016513: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (matts-computer): /proc/driver/nvidia/version does not exist\n",
      "2023-10-30 18:18:00.016777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "negatives = pd.read_csv(\"./negative.csv\")\n",
    "\n",
    "amino_acids = []\n",
    "\n",
    "def add_to_amino_acids(a_sequence: str):\n",
    "    for acid in a_sequence:\n",
    "        if acid not in amino_acids:\n",
    "            amino_acids.append(acid)\n",
    "\n",
    "positives.stack().reset_index(drop=True).apply(add_to_amino_acids)\n",
    "\n",
    "amino_acids.sort()\n",
    "\n",
    "amino_acid_label_encoder = LabelEncoder()\n",
    "amino_acid_label_encoder.fit(amino_acids)\n",
    "\n",
    "all_amino_acids = amino_acid_label_encoder.transform(amino_acids)\n",
    "\n",
    "def feature_map(p_sequence):\n",
    "    return [tf.one_hot(amino_acid_label_encoder.transform(list(x)), len(all_amino_acids)) for x in p_sequence]\n",
    "\n",
    "data_cd3r = feature_map(positives[\"cdr3\"])\n",
    "data_epitope = feature_map(positives[\"antigen.epitope\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"gene.jpg\" alt=\"Figure 1\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. CDR3B and Epitope Sequence Representation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Representation Goals:**\n",
    "\n",
    "  1. Convert amino acid sequences from string format to a numeric representation.\n",
    "  2. Develop a numerical procedure utilizing Atchley representation to capture physical and biochemical properties.\n",
    "  3. Create matrices with specified dimensions through padding to accommodate varying sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Construction with the Atchley Representation in both CDR3B and the Epitope\n",
    "Fixed: Instead of constructing the sentences manually, I constructed the sentences utilizing the BertTokenizer in relateion to the amino_aciv_vocab.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]],\n",
      "\n",
      "        [[2, 5, 6,  ..., 0, 0, 0]]])\n",
      "tensor([[[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2,  8, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]],\n",
      "\n",
      "        [[ 2, 10, 10,  ...,  0,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "def convert_to_space_separated_string(series):\n",
    "    return ' '.join(series)\n",
    "\n",
    "tokenizer = BertTokenizer(vocab_file=\"./amino_acid_vocab.txt\")\n",
    "\n",
    "def construct_sentences(dataframe):\n",
    "    cdr3_sentences = dataframe[\"cdr3\"]\n",
    "    epitope_sentences = dataframe[\"antigen.epitope\"]\n",
    "    return cdr3_sentences, epitope_sentences\n",
    "\n",
    "def pad_sentences(sentences, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    return torch.stack(input_ids), torch.stack(attention_masks)\n",
    "\n",
    "max_length = 32\n",
    "\n",
    "positives = pd.read_csv(\"./positive.csv\")\n",
    "\n",
    "for column in positives.columns:\n",
    "    positives[column] = positives[column].apply(convert_to_space_separated_string)\n",
    "\n",
    "cdr3_sentences, epitope_sentences = construct_sentences(positives)\n",
    "\n",
    "cdr3_input_ids, cdr3_attention_masks = pad_sentences(cdr3_sentences, max_length)\n",
    "epitope_input_ids, epitope_attention_masks = pad_sentences(epitope_sentences, max_length)\n",
    "\n",
    "cdr3_combined = torch.cat((cdr3_input_ids, cdr3_attention_masks), dim=1)\n",
    "epitope_combined = torch.cat((epitope_input_ids, epitope_attention_masks), dim=1)\n",
    "\n",
    "cdr3_train_data, cdr3_test_data = train_test_split(cdr3_combined, test_size=0.2, random_state=42)\n",
    "epitope_train_data, epitope_test_data = train_test_split(epitope_combined, test_size=0.2, random_state=42)\n",
    "print(cdr3_input_ids)\n",
    "print(epitope_input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure encoding CDR3B and Eptiope Amino Acid Sequences as Numerical Matrices\n",
    "#### Taken from Matt's code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskedLMOutput(loss=None, logits=tensor([[[ 0.0000,  0.1290, -0.1712,  ..., -0.1345, -0.0106, -0.0524],\n",
       "         [ 0.0000, -0.0574,  0.1205,  ..., -0.1356, -0.1956,  0.0307],\n",
       "         [ 0.0000,  0.0640, -0.0882,  ..., -0.1235,  0.0558, -0.1000],\n",
       "         ...,\n",
       "         [ 0.0000,  0.1143,  0.0363,  ..., -0.1049, -0.1265,  0.0897],\n",
       "         [ 0.0000,  0.1627,  0.0230,  ...,  0.0143, -0.0678,  0.0448],\n",
       "         [ 0.0000,  0.1746, -0.0648,  ..., -0.0725,  0.0789,  0.0503]],\n",
       "\n",
       "        [[ 0.0000,  0.0524, -0.2114,  ..., -0.1261, -0.0148, -0.0675],\n",
       "         [ 0.0000, -0.0600,  0.1611,  ..., -0.1093, -0.1943,  0.0145],\n",
       "         [ 0.0000, -0.0087, -0.0788,  ..., -0.1019,  0.0794, -0.1122],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0559,  0.0750,  ..., -0.1248, -0.2235,  0.0278],\n",
       "         [ 0.0000,  0.1855, -0.0010,  ..., -0.0070, -0.0348,  0.0621],\n",
       "         [ 0.0000,  0.1914, -0.0860,  ..., -0.1114,  0.0447, -0.0004]]],\n",
       "       grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "\n",
    "config = BertConfig.from_json_file(\"./bert_config.json\")\n",
    "model = BertForMaskedLM(config=config)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cdr3_train_data = cdr3_train_data.to(device)\n",
    "cdr3_test_data = cdr3_test_data.to(device)\n",
    "epitope_train_data = epitope_train_data.to(device)\n",
    "epitope_test_data = epitope_test_data.to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model(cdr3_train_data[:2][:,0], cdr3_train_data[:2][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/matthandzel/Code/Python/JimmyProject/BERT_1 copy/BERT_1 copy.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m       optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m       \u001b[39mprint\u001b[39m(loss)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m train_model(cdr3_train_data, cdr3_test_data, epitope_train_data, epitope_test_data)\n",
      "\u001b[1;32m/home/matthandzel/Code/Python/JimmyProject/BERT_1 copy/BERT_1 copy.ipynb Cell 15\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(cdr3_train_data[:,\u001b[39m0\u001b[39m], cdr3_train_data[:,\u001b[39m1\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matthandzel/Code/Python/JimmyProject/BERT_1%20copy/BERT_1%20copy.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:244\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    235\u001b[0m inputs \u001b[39m=\u001b[39m (\n\u001b[1;32m    236\u001b[0m     (inputs,)\n\u001b[1;32m    237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    243\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[0;32m--> 244\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    245\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[1;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    119\u001b[0m         )\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating_point:\n\u001b[1;32m    121\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "def train_model(cdr3_train, cdr3_test, epitope_train, epitope_test, optim = torch.optim.Adam(model.parameters(), lr=1e-5)):\n",
    "    model.train()\n",
    "    for epoch in range(1):\n",
    "      optim.zero_grad()\n",
    "      outputs = model(cdr3_train_data[:,0], cdr3_train_data[:,1])\n",
    "      loss = outputs[0]\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      print(loss)\n",
    "train_model(cdr3_train_data, cdr3_test_data, epitope_train_data, epitope_test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
